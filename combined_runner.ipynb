{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader, UnstructuredPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain.schema import Document\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from unstructured.embed.huggingface import HuggingFaceEmbeddingEncoder, HuggingFaceEmbeddingConfig\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "# from llm_client_chat import AlpacaLLM\n",
    "from llm_client import AlpacaLLM\n",
    "import time\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHROMA_PATH = \"langchain-loader/chroma-multilingual_e5_large-semantic-split\"\n",
    "# CHROMA_PATH = \"unstructured-loader/chroma-multilingual_e5_title_split\"\n",
    "DATA_PATH = \"data/pdfs\"\n",
    "# os.environ['OPENAI_API_KEY'] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embedding_model():\n",
    "    start_time = time.time()\n",
    "    model_path=\"intfloat/multilingual-e5-large\"   \n",
    "    encode_kwargs = {\"normalize_embeddings\": True}\n",
    "    local_embedding = HuggingFaceEmbeddings(\n",
    "        model_name=model_path,\n",
    "        cache_folder=\"./models\",\n",
    "        encode_kwargs=encode_kwargs\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    print(f'model load time {round(end_time - start_time, 0)} second')\n",
    "    return local_embedding\n",
    "\n",
    "embedding = load_embedding_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents(path = DATA_PATH):\n",
    "    loader = DirectoryLoader(path, \n",
    "                             glob=\"*.pdf\",\n",
    "                             loader_cls=UnstructuredPDFLoader)\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "\n",
    "def split_text(documents: list[Document]):\n",
    "    print(\"Starting chunking\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    \"Character Splitter\"\n",
    "    text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\\n\",\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=300,\n",
    "    length_function=len,\n",
    "    strip_whitespace=True\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "    \"Recursive Splitter\"\n",
    "    # text_splitter = RecursiveCharacterTextSplitter(\n",
    "    #     chunk_size=500,\n",
    "    #     chunk_overlap=100,\n",
    "    #     length_function=len,\n",
    "    #     add_start_index=True,\n",
    "    # )\n",
    "    # chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "    \"Semantic Splitter\"\n",
    "    # text_splitter = SemanticChunker(embedding)\n",
    "    # chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    print(f\"Split {len(documents)} documents into {len(chunks)} chunks.\")\n",
    "    print(f\"len docs {len(chunks)}\")\n",
    "    document = chunks[10]\n",
    "    print(\"page content \\n\", document.page_content)\n",
    "    print(\"doc metadata \\n\", document.metadata)\n",
    "    print(f'chunking time {round(end_time - start_time, 0)} second')\n",
    "    return chunks\n",
    "\n",
    "def save_to_chroma(chunks: list[Document]):\n",
    "    print(\"Starting Embedding\")\n",
    "    # Clear out the database first.\n",
    "    if os.path.exists(CHROMA_PATH):\n",
    "        shutil.rmtree(CHROMA_PATH)\n",
    "\n",
    "    # Create a new DB from the documents.\n",
    "    start_time = time.time()\n",
    "    db = Chroma.from_documents(\n",
    "        chunks, \n",
    "        embedding, \n",
    "        persist_directory=CHROMA_PATH,\n",
    "        collection_metadata={\"hnsw:space\": \"cosine\"}\n",
    "    )\n",
    "    db.persist()\n",
    "    end_time = time.time()\n",
    "    print(f'embedding time {round(end_time - start_time, 0)} second')\n",
    "    print(f\"Saved {len(chunks)} chunks to {CHROMA_PATH}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f\"data/pdfs/PNPK 2023 GGK.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/table-transformer-structure-recognition were not used when initializing TableTransformerForObjectDetection: ['model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Extracts the elements from the PDF\n",
    "elements = partition_pdf(\n",
    "    filename=filename,\n",
    "\n",
    "    # Unstructured Helpers\n",
    "    strategy=\"hi_res\", \n",
    "    infer_table_structure=True, \n",
    "    hi_res_model_name=\"yolox\",\n",
    "    extract_images_in_pdf=True,\n",
    "    extract_image_block_output_dir=\"PNPK 2023 GGK-Images\"\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This function will be deprecated in a future release and `unstructured` will simply use the DEFAULT_MODEL from `unstructured_inference.model.base` to set default model name\n"
     ]
    }
   ],
   "source": [
    "# Get pictures\n",
    "raw_pdf_elements = partition_pdf(\n",
    "    filename=filename,\n",
    "    \n",
    "    # Using pdf format to find embedded image blocks\n",
    "    \n",
    "    \n",
    "    # Use layout model (YOLOX) to get bounding boxes (for tables) and find titles\n",
    "    # Titles are any sub-section of the document\n",
    "    infer_table_structure=True,\n",
    "    \n",
    "    # Post processing to aggregate text once we have the title\n",
    "    chunking_strategy=\"by_title\",\n",
    "    # Chunking params to aggregate text blocks\n",
    "    # Attempt to create a new chunk 3800 chars\n",
    "    # Attempt to keep chunks > 2000 chars\n",
    "    # Hard max on chunks\n",
    "    max_characters=4000,\n",
    "    new_after_n_chars=3800,\n",
    "    combine_text_under_n_chars=2000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "clean_elements = []\n",
    "\n",
    "pattern = r'\\d+ -'\n",
    "\n",
    "for element in elements:\n",
    "    if not (re.match(pattern, element.__str__()) or element.__str__() == \"jdih.kemkes.go.id\"):\n",
    "        clean_elements.append(element)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unstructured.chunking.basic import chunk_elements\n",
    "from unstructured.chunking.title import chunk_by_title\n",
    "\n",
    "chunks = chunk_elements(clean_elements, \n",
    "                        max_characters=1500, \n",
    "                        new_after_n_chars=1000, \n",
    "                        overlap=300, \n",
    "                        overlap_all=True)\n",
    "\n",
    "chunks = chunk_by_title(clean_elements,\n",
    "                        combine_text_under_n_chars = 500,\n",
    "                        overlap=300, \n",
    "                        overlap_all=True)\n",
    "\n",
    "clean_elements_str_lst = [element.__str__() for element in chunks]\n",
    "\n",
    "if os.path.exists(CHROMA_PATH):\n",
    "        shutil.rmtree(CHROMA_PATH)\n",
    "\n",
    "db = Chroma.from_texts(\n",
    "        clean_elements_str_lst, \n",
    "        embedding, \n",
    "        persist_directory=CHROMA_PATH, \n",
    "        collection_metadata={\"hnsw:space\": \"cosine\"}\n",
    "    )\n",
    "db.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = load_documents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'data\\\\pdfs\\\\PNPK 2023 GGK.pdf'}\n"
     ]
    }
   ],
   "source": [
    "print(documents[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = split_text(documents)\n",
    "save_to_chroma(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Jawablah pertanyaan di bawah hanya menggunakan informasi di bawah ini, masing-masing informasi dipisahkan oleh '---':\n",
    "\n",
    "---\n",
    "\n",
    "{context}\n",
    "\n",
    "---\n",
    "\n",
    "Jawab pertanyaan menggunakan informasi di atas: {question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------\n",
      "Apa komplikasi yang dapat terjadi pada anak dengan PGK?\n",
      "1. doc: \n",
      "lah kelainan ginjal akibat infeksi atau kelainan yang didapat, di mana anak dirujuk saat sudah pada stadium lanjut dari PGK. Data epidemiologi anak PGK di negara berkembang tidak pasti dan tidak mencerminkan kondisi sesungguhnya karena biasanya pendataan baru dimulai saat anak sudah mulai dialisis. PGK pada anak mempunyai karakteristik dan tantangan yang khas dan unik yang tidak dijumpai pada pasien PGK dewasa. Masalah khusus yang dijumpai pada PGK anak antara lain gangguan pertumbuhan dan nutrisi serta masalah psikososial yang mempengaruhi kualitas hidup anak secara bermakna, termasuk fungsi ginjalnya. Penanganan pasien PGK anak harus memikirkan masa depannya, sedangkan pada dewasa justru harus memikirkan masa lalu riwayat hidup sebelumnya. Oleh karena itu berbagai organisasi internasional seperti The Centers for Disease Control and Prevention (CDC) dan World Health Organization (WHO) telah menetapkan berbagai strategi untuk mencegah perkembangan, progresivitas dan komplikasi PGK pada, \n",
      "metadata: {}, \n",
      "score: 0.8972715139389038\n",
      "\n",
      "2. doc: \n",
      "arkan dengan 1 gram sodium polystyrene sulphonate. Dosis oral untuk bayi dan anak adalah 1 g/kg/dosis (dosis maksimal 15 g/dosis) tiap 6 jam. Terapi ini dapat menyebabkan diare dan nekrosis kolon sehingga keseimbangan cairan harus dipantau ketat selama penggunaan\n",
      "\n",
      "terapi ini.\n",
      "\n",
      "c. Asidosis Metabolik Asidosis metabolik merupakan komplikasi PGK pada anak yang terjadi secara dini dan sering dijumpai, terutama karena tingginya prevalensi kelainan kongenital ginjal dan saluran kemih serta kelainan terkait disfungsi tubulus ginjal. Kadar bikarbonat ≤22 mEq/L pada anak PGK dengan penyebab primer kelainan glomerular dan ≤18 mEq/L pada anak PGK dengan penyebab primer kelainan non-glomerular terbukti meningkatkan risiko terhadap percepatan progresivitas PGK sebesar 54%. Selain itu, kadar bikarbonat yang rendah pada anak PGK dapat menyebabkan gangguan pertumbuhan pada kadar bikarbonat ≤18 mEq/L. Suplementasi bikarbonat perlu diberikan pada anak PGK untuk mempertahankan kadar bikarbonat serum 24-26, \n",
      "metadata: {}, \n",
      "score: 0.8925432562828064\n",
      "\n",
      "3. doc: \n",
      "dengan PGK, biasanya menyebabkan kondisi amenore pada saat pasien masuk ke dalam PGTA.\n",
      "\n",
      "Implikasi klinis penting dari kelainan ini adalah bahwa kehamilan hingga aterm jarang terjadi pada wanita dengan konsentrasi kreatinin serum 3 mg/dL (265 mikromol/L). [Peringkat bukti: Level IV]\n",
      "\n",
      "Rekomendasi 13:\n",
      "\n",
      "Terdapat beberapa upaya tatalaksana terhadap komplikasi yang terjadi akibat PGK, yaitu:\n",
      "\n",
      "1) Overhidrasi: Dapat dipertimbangkan untuk pemberian terapi loop diuretik dengan pembatasan diet natrium.\n",
      "\n",
      "2) Hiperkalemia: Dapat dipertimbangkan untuk diet rendah kalium, menghindari dalam penggunaan obat-obatan yang dapat meningkatkan kadar kalium serum, serta mengevaluasi kadar kalium serum dan nilai eGFR pada pasien dengan terapi ACE-I atau ARB.\n",
      "\n",
      "3) Asidosis metabolik: Pada orang dewasa dengan PGK 3- 5D, agar mempertahankan kadar bikarbonat serum pada\n",
      "\n",
      "kadar 24-26 mmol/L., \n",
      "metadata: {}, \n",
      "score: 0.8921383619308472\n",
      "\n",
      "-----------------------------------------------------\n",
      "\n",
      "-----------------------------------------------------\n",
      "EXITR\n",
      "1. doc: \n",
      "rjadinya exit site infection direkomendasikan hal-hal sebagai berikut:149,161,162\n",
      "\n",
      "a) Pasien tidak diperkenankan untuk berenang di sungai atau danau, berendam di air panas, jacuzzi, sauna, atau berenang di kolam renang umum. Hal tersebut berpotensi untuk terjadinya infeksi oleh bakteri Gram negatif.\n",
      "\n",
      "b) Penggunaan perban hanya disarankan jika exit site kotor atau basah. Perban harus segera diganti jika kotor atau basah.\n",
      "\n",
      "c) Penggunaan larutan antimikroba dan salep mupirocin atau gentamicin secara rutin terbukti mampu mencegah terjadinya komplikasi infeksi\n",
      "\n",
      "d) Kateter pada exit site harus dibersihkan setiap tampak kotor. Direkomendasikan membersihkan minimal 2x seminggu dan setiap habis mandi.\n",
      "\n",
      "Perawatan kateter exit site dan tunnel merupakan hal yang wajib dilakukan untuk menurunkan risiko peritonitis. Secara umum, perawatan exit site dan tunnel dapat dilakukan sebagai berikut:, \n",
      "metadata: {}, \n",
      "score: 0.7710201144218445\n",
      "\n",
      "2. doc: \n",
      "ganism Exit- Sites Tunnel Infections (POET) antara tahun 1996 dan 2005, dengan hubungan antara jumlah manset kateter dan kejadian peritonitis. Kesimpulan dari penelitian ini adalah penggunaan kateter DP double-cuff dikaitkan dengan penurunan peritonitis akibat S. aureus.\n",
      "\n",
      "[Peringkat bukti: Level II], \n",
      "metadata: {}, \n",
      "score: 0.7663735151290894\n",
      "\n",
      "3. doc: \n",
      "gkatan pembuangan cairan dialitik. Pada akhirnya, ISPD Ad Hoc Committee on Ultrafiltration Management in Peritoneal Dialysis and the European Renal Best Practice working group merekomendasikan agar ikodekstrin digunakan untuk waktu lama pada pasien transporter tinggi dengan ultrafiltrasi peritoneal bersih <400 mL selama Peritoneal Equilibration Test (PET), berlangsung, \n",
      "metadata: {}, \n",
      "score: 0.7639210224151611\n",
      "\n",
      "-----------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chat = True\n",
    "while chat:\n",
    "    # Accept user input\n",
    "    user_prompt = input(\"User > \")\n",
    "    if user_prompt == \"exit\":\n",
    "        chat = False\n",
    "        continue\n",
    "    else :\n",
    "        pass\n",
    "    \n",
    "    # load DB.\n",
    "    db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding)\n",
    "\n",
    "    # Search the DB.\n",
    "    results = db.similarity_search_with_relevance_scores(user_prompt, k=3)\n",
    "    if len(results) == 0 or results[0][1] < 0.7:\n",
    "        print(f\"Similarity too low.\", end=\"\\n\")\n",
    "        # return\n",
    "\n",
    "    context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _score in results])\n",
    "    prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "    prompt = prompt_template.format(context=context_text, question=user_prompt)\n",
    "\n",
    "    # print(prompt)\n",
    "    counter = 1\n",
    "    print(\"-----------------------------------------------------\")\n",
    "    print(user_prompt)\n",
    "    for doc, _score in results:\n",
    "        print(f\"{counter}. doc: \\n{doc.page_content}, \\nmetadata: {doc.metadata}, \\nscore: {_score}\\n\")\n",
    "        counter+=1\n",
    "    print(\"-----------------------------------------------------\")\n",
    "    print()\n",
    "\n",
    "    # model = AlpacaLLM()\n",
    "    # response_text = model.invoke(prompt)\n",
    "\n",
    "    # # model = ChatOpenAI()\n",
    "    # # response_text = model.predict(prompt)\n",
    "\n",
    "    # sources = [doc.metadata.get(\"source\", None) for doc, _score in results]\n",
    "    # print()\n",
    "    # formatted_response = f\"AI > Response: {response_text}\\nSources: {sources}\"\n",
    "    # # formatted_response = f\"Response: {response_text}\"\n",
    "    # print(formatted_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
